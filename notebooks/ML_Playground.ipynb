{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, I'll import the necessary libraries into the project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the necessary imports for the project to work\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this section I will retrieve the data to be used to train and evaluate the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection and reading the data\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "df_disaster_categories = pd.read_sql_table('disaster_categories', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before building the model, I will split the data into predictor and target variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = df_disaster_categories.message\n",
    "y = df_disaster_categories.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some adjustments to the data are required as the predictor columns are in text format. Also, I need to check if there is any column without variability, that is, with all values ​​equal.**\n",
    "\n",
    "**To do this, two steps will be performed:** \n",
    "1. **Tokenization on the messages column.** \n",
    "2. **Search and remove columns without variability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, a function will be created to clear the text data. The steps are:**\n",
    "1. **remove stopwords**\n",
    "2. **reduce words to their root form**\n",
    "3. **normalize case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove punctuation and urls\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    new_text = re.sub(url_regex, ' ', text)\n",
    "    new_text = re.sub('\\W', ' ', new_text)\n",
    "\n",
    "    # Removing stopwords, bringing words to their root form and normalize them\n",
    "    tokens = word_tokenize(new_text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tokens = [lemmatizer.lemmatize(word).lower().strip() for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Searching and removing columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching columns without variability\n",
    "y.columns[y.sum() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.drop(columns=y.columns[y.sum() == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building and training a Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To assemble the pipeline, I will use the ``CountVectorizer`` to transform the tokenized texts with the function created into vectors, the ``TfidfTransformer`` to weight the variables, finally, I will train a ``decision tree`` with this data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=.30,\n",
    "    random_state=41\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the pipeline\n",
    "pipe = Pipeline([('vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the pipeline\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the metrics\n",
    "print(classification_report(y_test, pipe.predict(X_test), target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The metrics weren't good, let's try to tune the hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tunning hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters\n",
    "params = {'clf__max_depth':[6, 7, 15],\n",
    "          'clf__min_samples_split':[20, 30, 40],\n",
    "          'clf__criterion':['entropy', 'gini']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for the best combination\n",
    "grid_clf = GridSearchCV(pipe, params, cv = 5) \n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the metrics\n",
    "print(classification_report(y_test, grid_clf.predict(X_test), target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the end, the score remained low, making it necessary to add more data, or change the approach when processing the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Saving the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid_clf, 'model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7b2ef02f2a098e2398fa32f175f435f106ef85c0862e86e9e2150442a852cab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
